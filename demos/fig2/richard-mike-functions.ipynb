{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def cef_estimate(X, y, n_estimators = 200, max_samples = .32, bootstrap = True, depth = 30, min_samples_leaf = 1, max_features = 1.):\n",
    "#     model = BaggingClassifier(DecisionTreeClassifier(max_depth = depth, min_samples_leaf = min_samples_leaf, max_features = math.ceil(int(math.sqrt(X.shape[1])))), \n",
    "#                               n_estimators = n_estimators, \n",
    "#                               max_samples= max_samples, \n",
    "#                               bootstrap = bootstrap)\n",
    "#     model.fit(X, y)\n",
    "#     class_counts = np.zeros((X.shape[0], model.n_classes_))\n",
    "#     for idx, tree in enumerate(model): # RONAK EDIT\n",
    "#         # get out of bag indicies\n",
    "        \n",
    "#         # RONAK EDIT STARTS HERE ################ In newer sklearn, generate unsampled takes a positional argument.\n",
    "#         #unsampled_indices = _generate_unsampled_indices(tree.random_state, len(X))\n",
    "#         sampled_indices = model.estimators_samples_[idx]\n",
    "#         unsampled_indices = np.delete(np.arange(0, len(X)), sampled_indices)\n",
    "        \n",
    "#         # RONAK EDIT ENDS HERE ##################\n",
    "        \n",
    "#         total_unsampled = len(unsampled_indices)\n",
    "#         np.random.shuffle(unsampled_indices)\n",
    "#         prob_indices, eval_indices = unsampled_indices[:total_unsampled//2], unsampled_indices[total_unsampled//2:]\n",
    "#         # get all node counts\n",
    "#         node_counts = tree.tree_.n_node_samples\n",
    "#         # get probs for eval samples\n",
    "#         posterior_class_counts = np.zeros((len(node_counts), model.n_classes_))\n",
    "#         for prob_index in prob_indices:\n",
    "#             posterior_class_counts[tree.apply(X[prob_index].reshape(1, -1)).item(), y[prob_index]] += 1\n",
    "#         row_sums = posterior_class_counts.sum(axis=1)\n",
    "#         row_sums[row_sums == 0] = 1\n",
    "#         class_probs = (posterior_class_counts/row_sums[:, None])\n",
    "        \n",
    "#         where_0 = np.argwhere(class_probs == 0)\n",
    "#         for elem in where_0:\n",
    "#             class_probs[elem[0], elem[1]] = 1/(2*row_sums[elem[0], None])\n",
    "#         where_1 = np.argwhere(class_probs == 1)\n",
    "#         for elem in where_1:\n",
    "#             class_probs[elem[0], elem[1]] = 1 - 1/(2*row_sums[elem[0], None])\n",
    "        \n",
    "#         class_probs.tolist()\n",
    "#         partition_counts = np.asarray([node_counts[x] for x in tree.apply(X[eval_indices])])\n",
    "#         # get probability for out of bag samples\n",
    "#         eval_class_probs = [class_probs[x] for x in tree.apply(X[eval_indices])]\n",
    "#         eval_class_probs = np.array(eval_class_probs)\n",
    "#         # find total elements for out of bag samples\n",
    "#         elems = np.multiply(eval_class_probs, partition_counts[:, np.newaxis])\n",
    "#         # store counts for each x (repeat fhis for each tree)\n",
    "#         class_counts[eval_indices] += elems\n",
    "#     # calculate p(y|X = x) for all x's\n",
    "#     probs = class_counts/class_counts.sum(axis = 1, keepdims = True)\n",
    "#     entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "#     # convert nan to 0\n",
    "#     entropies = np.nan_to_num(entropies)\n",
    "#     return np.mean(entropies)\n",
    "\n",
    "# np.warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cef_estimate_mike_1(X, y, n_estimators = 200, max_samples = .32, bootstrap = True, depth = 30, min_samples_leaf = 1, max_features = 1.):\n",
    "    model = BaggingClassifier(DecisionTreeClassifier(max_depth = depth, min_samples_leaf = min_samples_leaf, max_features = math.ceil(int(math.sqrt(X.shape[1])))), \n",
    "                              n_estimators = n_estimators, \n",
    "                              max_samples= max_samples, \n",
    "                              bootstrap = bootstrap)\n",
    "    model.fit(X, y)\n",
    "    class_counts = np.zeros((X.shape[0], model.n_classes_))\n",
    "    tree_idx = 0\n",
    "    for tree in model:\n",
    "        \n",
    "        # get out of bag indices.       \n",
    "        # Here's where we obtain unsampled indices.\n",
    "        # unsampled_indices = _generate_unsampled_indices(tree.random_state, len(X), int((1 - max_samples)*len(X)))\n",
    "        sampled_indices = model.estimators_samples_[tree_idx]\n",
    "        unsampled_indices = np.delete(np.arange(0,X.shape[0]), sampled_indices)\n",
    "        tree_idx = tree_idx + 1\n",
    "        # Done with unsampled indices.\n",
    "        \n",
    "        total_unsampled = len(unsampled_indices)\n",
    "        np.random.shuffle(unsampled_indices)\n",
    "        prob_indices, eval_indices = unsampled_indices[:total_unsampled//2], unsampled_indices[total_unsampled//2:]\n",
    "        # get all node counts\n",
    "        node_counts = tree.tree_.n_node_samples\n",
    "        # get probs for eval samples\n",
    "        posterior_class_counts = np.zeros((len(node_counts), model.n_classes_))\n",
    "        for prob_index in prob_indices:\n",
    "            posterior_class_counts[tree.apply(X[prob_index].reshape(1, -1)).item(), y[prob_index]] += 1\n",
    "        row_sums = posterior_class_counts.sum(axis=1)\n",
    "        row_sums[row_sums == 0] = 1\n",
    "        class_probs = (posterior_class_counts/row_sums[:, None])\n",
    "        \n",
    "        where_0 = np.argwhere(class_probs == 0)\n",
    "        for elem in where_0:\n",
    "            class_probs[elem[0], elem[1]] = 1/(2*row_sums[elem[0], None])\n",
    "        where_1 = np.argwhere(class_probs == 1)\n",
    "        for elem in where_1:\n",
    "            class_probs[elem[0], elem[1]] = 1 - 1/(2*row_sums[elem[0], None])\n",
    "        \n",
    "        class_probs.tolist()\n",
    "        partition_counts = np.asarray([node_counts[x] for x in tree.apply(X[eval_indices])])\n",
    "        # get probability for out of bag samples\n",
    "        eval_class_probs = [class_probs[x] for x in tree.apply(X[eval_indices])]\n",
    "        eval_class_probs = np.array(eval_class_probs)\n",
    "        # find total elements for out of bag samples\n",
    "        elems = np.multiply(eval_class_probs, partition_counts[:, np.newaxis])\n",
    "        # store counts for each x (repeat fhis for each tree)\n",
    "        class_counts[eval_indices] += elems\n",
    "    # calculate p(y|X = x) for all x's\n",
    "    probs = class_counts/class_counts.sum(axis = 1, keepdims = True)\n",
    "    entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "    # convert nan to 0\n",
    "    entropies = np.nan_to_num(entropies)\n",
    "    return np.mean(entropies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
